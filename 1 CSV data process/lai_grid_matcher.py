"""
è¯¥è„šæœ¬ç”¨äºåŒ¹é…ç½‘æ ¼:
åŸºäº"ç©ºä»£æ—¶"çš„æ€æƒ³, å°†æ‰€æœ‰æ—¶é—´æ®µçš„5kmæ ¼ç½‘è¿›è¡ŒåŒ¹é…

ä¿®æ”¹ç‰ˆï¼šæ”¯æŒç”¨æˆ·æŒ‡å®šåŒºåŸŸã€æ¨¡å¼å’Œå­£èŠ‚è¿›è¡ŒåŒ¹é…ï¼Œå¹¶æŒ‰å­£èŠ‚åˆ†åˆ«å¯¼å‡ºç»“æœ

1. å››ç§åŒ¹é…æ¨¡å¼ï¼š
   - Mode1: ç›¸åŒå­£èŠ‚å†…çš„ç›¸åå˜åŒ–åŒ¹é…
   - Mode2: ä¸åŒå­£èŠ‚å†…çš„ç›¸åŒå˜åŒ–åŒ¹é…
   - Mode3: ç›¸åŒå­£èŠ‚å†…çš„ç›¸åŒå˜åŒ–åŒ¹é…
   - Mode4: ä¸åŒå­£èŠ‚å†…çš„ç›¸åå˜åŒ–åŒ¹é…

2. è¾“å‡ºæ–‡ä»¶ç»“æ„ï¼š
   matching_results/
   â”œâ”€â”€ Boreal/
   â”‚   â”œâ”€â”€ Spring_mode1.csv       # å•ä¸ªå­£èŠ‚åŒ¹é…
   â”‚   â”œâ”€â”€ Summer_mode1.csv       # å•ä¸ªå­£èŠ‚åŒ¹é…
   â”‚   â”œâ”€â”€ Spring_Summer_mode2.csv # ä¸åŒå­£èŠ‚åŒ¹é…
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ Temperate/...
   â”œâ”€â”€ Tropical/...
   â”œâ”€â”€ matching_summary_*.csv     # æ€»ä½“ç»Ÿè®¡
   â””â”€â”€ region_summary_*.csv       # åŒºåŸŸç»Ÿè®¡
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from tqdm import tqdm
from typing import List, Tuple, Dict, Iterator, Optional
from datetime import datetime
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
from itertools import combinations
from collections import defaultdict

warnings.filterwarnings('ignore')


class OptimizedGridMatchingAnalyzer:
    """ä¼˜åŒ–ç‰ˆç½‘æ ¼åŒ¹é…åˆ†æå™¨ - é«˜æ€§èƒ½å®ç°ï¼ŒæŒ‰å­£èŠ‚åˆ†åˆ«å¯¼å‡º"""

    def __init__(self, input_dir: str, output_dir: str, chunk_size: int = 50000):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        :param input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        :param output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        :param chunk_size: åˆ†å—å¤„ç†å¤§å°, ç”¨äºæ§åˆ¶å†…å­˜ä½¿ç”¨
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.chunk_size = chunk_size

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(__name__)

        # LAIå˜åŒ–ç‡åŒºé—´å®šä¹‰
        self.rate_intervals = np.array([0.0, 0.2, 0.4, 0.6, 0.8, np.inf])

        # ä¿®æ­£æ•°æ®ç±»å‹ä¼˜åŒ–é…ç½® - æ ¹æ®å®é™…æ•°æ®ç»“æ„
        self.dtypes_config = {
            'large_grid_id': 'int32',  # large_grid_idæ˜¯æ•´æ•°
            # grid_idæ˜¯å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦è½¬æ¢
            'grid_type': 'int8',
            'LAI_Change_Rate_mean': 'float32',
            'LAI_Begin_mean': 'float32',
            'region': 'category',
            'year_pair': 'category',
            'season': 'category',  # å°å†™çš„season
            'analysis_group': 'category'
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'total_records': 0,
            'regions': set(),
            'year_pairs': set(),
            'seasons': set()
        }

    def optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¼˜åŒ–DataFrameçš„æ•°æ®ç±»å‹, ä»¥å‡å°‘å†…å­˜å ç”¨
        """
        # åº”ç”¨æ•°æ®ç±»å‹ä¼˜åŒ–
        for col, dtype in self.dtypes_config.items():
            if col in df.columns:
                try:
                    if dtype == 'category':
                        df[col] = df[col].astype('category')
                    else:
                        # å¯¹æ•°å€¼åˆ—è¿›è¡Œè½¬æ¢
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        # åˆ é™¤è½¬æ¢å¤±è´¥çš„è¡Œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                        if df[col].isnull().any():
                            before_count = len(df)
                            df = df.dropna(subset=[col])
                            after_count = len(df)
                            if before_count > after_count:
                                self.logger.warning(f"åˆ é™¤äº† {before_count - after_count} è¡ŒåŒ…å«æ— æ•ˆ {col} çš„æ•°æ®")
                        df[col] = df[col].astype(dtype)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ— æ³•è½¬æ¢åˆ— {col} ä¸º {dtype}: {e}")

        # ç‰¹æ®Šå¤„ç†grid_idï¼ˆä¿æŒä¸ºå­—ç¬¦ä¸²ï¼‰
        if 'grid_id' in df.columns:
            df['grid_id'] = df['grid_id'].astype(str)

        # æ™ºèƒ½ä¼˜åŒ–å…¶ä»–æ•°å€¼åˆ—
        for col in df.select_dtypes(include=['float64']).columns:
            if col not in self.dtypes_config:
                try:
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')
                except:
                    continue

        for col in df.select_dtypes(include=['int64']).columns:
            if col not in self.dtypes_config:
                try:
                    col_min, col_max = df[col].min(), df[col].max()
                    if pd.isna(col_min) or pd.isna(col_max):
                        continue
                    if col_min >= -128 and col_max <= 127:
                        df[col] = df[col].astype('int8')
                    elif col_min >= -32768 and col_max <= 32767:
                        df[col] = df[col].astype('int16')
                    else:
                        df[col] = df[col].astype('int32')
                except:
                    continue

        return df

    def load_single_file(self, file_path: Path) -> pd.DataFrame:
        """
        åŠ è½½å•ä¸ªæ–‡ä»¶è¿›è¡Œé¢„å¤„ç†
        """
        try:
            # ä»è·¯å¾„æå–ä¿¡æ¯
            parts = file_path.parts
            if len(parts) >= 3:
                region_from_path = parts[-3]
                year_pair_from_path = parts[-2]
                season_from_filename = file_path.stem.lower().capitalize()
            else:
                region_from_path = year_pair_from_path = season_from_filename = 'unknown'

            # æ ‡å‡†åŒ–å­£èŠ‚åç§°
            season_mapping = {
                'Spring': 'Spring', 'Summer': 'Summer',
                'Autumn': 'Autumn', 'Winter': 'Winter',
                'Fall': 'Autumn'
            }
            season_from_filename = season_mapping.get(season_from_filename, season_from_filename)

            # è¯»å–CSVæ–‡ä»¶ï¼Œä¸å¼ºåˆ¶æŒ‡å®šæ•°æ®ç±»å‹
            df = pd.read_csv(file_path, low_memory=False)

            # å¦‚æœæ•°æ®ä¸­æ²¡æœ‰regionæˆ–year_pairåˆ—ï¼Œä»è·¯å¾„è¡¥å……
            if 'region' not in df.columns:
                df['region'] = region_from_path
            if 'year_pair' not in df.columns:
                df['year_pair'] = year_pair_from_path

            # å¤„ç†å­£èŠ‚åˆ— - ç»Ÿä¸€ä¸ºå¤§å†™çš„Seasonç”¨äºåŒ¹é…
            if 'season' in df.columns:
                # æ•°æ®ä¸­æœ‰å°å†™çš„seasonï¼Œåˆ›å»ºå¤§å†™ç‰ˆæœ¬
                df['Season'] = df['season'].str.capitalize()
            elif 'Season' not in df.columns:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä»æ–‡ä»¶ååˆ›å»º
                df['Season'] = season_from_filename
                df['season'] = season_from_filename.lower()

            # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
            df['source_file'] = file_path.name
            df['season_from_path'] = season_from_filename

            # ä¼˜åŒ–æ•°æ®ç±»å‹
            df = self.optimize_dataframe(df)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['regions'].add(df['region'].iloc[0] if not df.empty else region_from_path)
            self.stats['year_pairs'].add(df['year_pair'].iloc[0] if not df.empty else year_pair_from_path)
            self.stats['seasons'].add(df['Season'].iloc[0] if not df.empty else season_from_filename)

            return df

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    def load_all_data_parallel(self) -> pd.DataFrame:
        """
        å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶
        """
        self.logger.info("ğŸ”„ å¼€å§‹å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶...")

        all_files = list(self.input_dir.rglob("*.csv"))
        if not all_files:
            raise FileNotFoundError(f"åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")

        self.stats['total_files'] = len(all_files)
        data_list = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŠ è½½
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self.load_single_file, file_path): file_path
                for file_path in all_files
            }

            # æ”¶é›†ç»“æœ
            for future in tqdm(as_completed(future_to_file),
                               total=len(all_files),
                               desc="ğŸ“ å¹¶è¡ŒåŠ è½½æ–‡ä»¶",
                               unit="ä¸ª"):
                try:
                    df = future.result()
                    if not df.empty:
                        data_list.append(df)
                except Exception as e:
                    file_path = future_to_file[future]
                    self.logger.error(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path.name}: {e}")

        if not data_list:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ•°æ®
        self.logger.info("ğŸ”— åˆå¹¶æ•°æ®...")
        global_df = pd.concat(data_list, ignore_index=True, copy=False)

        # æœ€ç»ˆä¼˜åŒ–
        global_df = self.optimize_dataframe(global_df)

        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['grid_id', 'Season', 'grid_type', 'LAI_Change_Rate_mean']
        missing_cols = [col for col in required_cols if col not in global_df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")

        # åˆ é™¤ä¸´æ—¶æ•°æ®é‡Šæ”¾å†…å­˜
        del data_list
        gc.collect()

        self.stats['total_records'] = len(global_df)

        self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {self.stats['total_files']} ä¸ªæ–‡ä»¶ï¼Œå…± {self.stats['total_records']:,} æ¡è®°å½•")
        self.logger.info(f"ğŸ“Š åŒºåŸŸåˆ†å¸ƒ: {sorted(self.stats['regions'])}")
        self.logger.info(f"ğŸ“Š å­£èŠ‚åˆ†å¸ƒ: {sorted(self.stats['seasons'])}")
        self.logger.info(f"ğŸ“Š å†…å­˜ä½¿ç”¨: {global_df.memory_usage(deep=True).sum() / 1024 ** 2:.1f} MB")

        return global_df

    def get_rate_interval_vectorized(self, rates: np.ndarray, use_abs: bool = True) -> np.ndarray:
        """
        å‘é‡åŒ–è®¡ç®—LAIå˜åŒ–ç‡åŒºé—´ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        values = np.abs(rates) if use_abs else rates
        # ä½¿ç”¨searchsortedæ›¿ä»£digitizeï¼Œæ€§èƒ½æ›´å¥½
        return np.searchsorted(self.rate_intervals[1:], values, side='right')

    def vectorized_matching_mode1(self, region_data: pd.DataFrame, region: str, target_season: Optional[str] = None) -> List[Tuple]:
        """
        å‘é‡åŒ–æ¨¡å¼1åŒ¹é… (ç›¸åŒå­£èŠ‚ç›¸åå˜åŒ–) - é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        matches = []

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œè¿‡æ»¤æ•°æ®
        if target_season:
            region_data = region_data[region_data['Season'] == target_season].copy()
            if len(region_data) == 0:
                self.logger.warning(f"âš ï¸ åœ¨{region}åŒºåŸŸä¸­æœªæ‰¾åˆ°{target_season}å­£èŠ‚çš„æ•°æ®")
                return matches

        # é¢„è®¡ç®—åŒºé—´
        rates = region_data['LAI_Change_Rate_mean'].values
        intervals = self.get_rate_interval_vectorized(rates, use_abs=True)
        region_data = region_data.copy()
        region_data['interval'] = intervals

        # æŒ‰(grid_id, Season)åˆ†ç»„
        grouped = region_data.groupby(['grid_id', 'Season'], observed=True)

        with tqdm(total=len(grouped),
                  desc=f"ğŸ”„ æ¨¡å¼1-ç›¸åŒå­£èŠ‚ç›¸åå˜åŒ–({region})",
                  unit="æ ¼ç½‘ç»„") as pbar:

            for (grid_id, season), group in grouped:
                if len(group) < 2:
                    pbar.update(1)
                    continue

                # ä½¿ç”¨å¸ƒå°”ç´¢å¼•å¿«é€Ÿåˆ†ç¦»
                type_1_mask = group['grid_type'] == 1
                type_neg1_mask = group['grid_type'] == -1

                if not (type_1_mask.any() and type_neg1_mask.any()):
                    pbar.update(1)
                    continue

                type_1_records = group[type_1_mask]
                type_neg1_records = group[type_neg1_mask]

                match_count = 0

                # å‘é‡åŒ–åŒºé—´åŒ¹é…
                for interval in range(len(self.rate_intervals) - 1):
                    t1_in_interval = type_1_records[type_1_records['interval'] == interval]
                    tn1_in_interval = type_neg1_records[type_neg1_records['interval'] == interval]

                    if len(t1_in_interval) > 0 and len(tn1_in_interval) > 0:
                        # ä½¿ç”¨numpy meshgridè¿›è¡Œå‘é‡åŒ–åŒ¹é…
                        idx1_array = t1_in_interval.index.values
                        idx2_array = tn1_in_interval.index.values

                        idx1_mesh, idx2_mesh = np.meshgrid(idx1_array, idx2_array)
                        for i1, i2 in zip(idx1_mesh.ravel(), idx2_mesh.ravel()):
                            matches.append((i1, i2, 'mode1', region, season))
                            match_count += 1

                pbar.set_postfix({"æ ¼ç½‘": f"{grid_id}", "å­£èŠ‚": season, "æ‰¾åˆ°": match_count})
                pbar.update(1)

        return matches

    def vectorized_matching_mode2(self, region_data: pd.DataFrame, region: str, target_seasons: Optional[List[str]] = None) -> List[Tuple]:
        """
        å‘é‡åŒ–æ¨¡å¼2åŒ¹é…ï¼ˆä¸åŒå­£èŠ‚ç›¸åŒå˜åŒ–ï¼‰- é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        matches = []

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œè¿‡æ»¤æ•°æ®
        if target_seasons and len(target_seasons) >= 2:
            region_data = region_data[region_data['Season'].isin(target_seasons)].copy()
            if len(region_data) == 0:
                self.logger.warning(f"âš ï¸ åœ¨{region}åŒºåŸŸä¸­æœªæ‰¾åˆ°æŒ‡å®šå­£èŠ‚çš„æ•°æ®")
                return matches

        # é¢„è®¡ç®—åŒºé—´
        rates = region_data['LAI_Change_Rate_mean'].values
        intervals = self.get_rate_interval_vectorized(rates, use_abs=False)
        region_data = region_data.copy()
        region_data['interval'] = intervals

        # æŒ‰grid_idåˆ†ç»„
        grouped = region_data.groupby('grid_id', observed=True)

        with tqdm(total=len(grouped),
                  desc=f"ğŸ”„ æ¨¡å¼2-ä¸åŒå­£èŠ‚ç›¸åŒå˜åŒ–({region})",
                  unit="æ ¼ç½‘") as pbar:

            for grid_id, group in grouped:
                if len(group) < 2:
                    pbar.update(1)
                    continue

                match_count = 0

                # æŒ‰grid_typeå†åˆ†ç»„
                for grid_type in [1, -1]:
                    same_type_records = group[group['grid_type'] == grid_type]

                    if len(same_type_records) < 2:
                        continue

                    # æŒ‰å­£èŠ‚åˆ†ç»„
                    season_groups = same_type_records.groupby('Season', observed=True)
                    seasons_list = list(season_groups)

                    # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œåªå¤„ç†è¿™äº›å­£èŠ‚
                    if target_seasons:
                        seasons_list = [(s, g) for s, g in seasons_list if s in target_seasons]

                    # ä¸åŒå­£èŠ‚é—´çš„å‘é‡åŒ–åŒ¹é…
                    for i in range(len(seasons_list)):
                        for j in range(i + 1, len(seasons_list)):
                            season1, group1 = seasons_list[i]
                            season2, group2 = seasons_list[j]

                            # å‘é‡åŒ–åŒºé—´åŒ¹é…
                            for interval in range(len(self.rate_intervals) - 1):
                                g1_interval = group1[group1['interval'] == interval]
                                g2_interval = group2[group2['interval'] == interval]

                                if len(g1_interval) > 0 and len(g2_interval) > 0:
                                    idx1_array = g1_interval.index.values
                                    idx2_array = g2_interval.index.values

                                    idx1_mesh, idx2_mesh = np.meshgrid(idx1_array, idx2_array)
                                    for i1, i2 in zip(idx1_mesh.ravel(), idx2_mesh.ravel()):
                                        matches.append((i1, i2, 'mode2', region, season1, season2))
                                        match_count += 1

                pbar.set_postfix({"æ ¼ç½‘": f"{grid_id}", "æ‰¾åˆ°": match_count})
                pbar.update(1)

        return matches

    def vectorized_matching_mode3(self, region_data: pd.DataFrame, region: str, target_season: Optional[str] = None) -> List[Tuple]:
        """
        å‘é‡åŒ–æ¨¡å¼3åŒ¹é…ï¼ˆç›¸åŒå­£èŠ‚ç›¸åŒå˜åŒ–ï¼‰- é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        matches = []

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œè¿‡æ»¤æ•°æ®
        if target_season:
            region_data = region_data[region_data['Season'] == target_season].copy()
            if len(region_data) == 0:
                self.logger.warning(f"âš ï¸ åœ¨{region}åŒºåŸŸä¸­æœªæ‰¾åˆ°{target_season}å­£èŠ‚çš„æ•°æ®")
                return matches

        # é¢„è®¡ç®—åŒºé—´
        rates = region_data['LAI_Change_Rate_mean'].values
        intervals = self.get_rate_interval_vectorized(rates, use_abs=False)
        region_data = region_data.copy()
        region_data['interval'] = intervals

        # æŒ‰(grid_id, Season)åˆ†ç»„
        grouped = region_data.groupby(['grid_id', 'Season'], observed=True)

        with tqdm(total=len(grouped),
                  desc=f"ğŸ”„ æ¨¡å¼3-ç›¸åŒå­£èŠ‚ç›¸åŒå˜åŒ–({region})",
                  unit="æ ¼ç½‘ç»„") as pbar:

            for (grid_id, season), group in grouped:
                if len(group) < 2:
                    pbar.update(1)
                    continue

                match_count = 0

                # æŒ‰grid_typeå†åˆ†ç»„
                for grid_type in [1, -1]:
                    same_type_records = group[group['grid_type'] == grid_type]

                    if len(same_type_records) < 2:
                        continue

                    # åœ¨ç›¸åŒåŒºé—´å†…è¿›è¡Œå‘é‡åŒ–ä¸¤ä¸¤åŒ¹é…
                    for interval in range(len(self.rate_intervals) - 1):
                        interval_records = same_type_records[same_type_records['interval'] == interval]

                        if len(interval_records) >= 2:
                            # ä½¿ç”¨combinationsè¿›è¡Œé«˜æ•ˆçš„ä¸¤ä¸¤ç»„åˆ
                            indices = interval_records.index.values
                            for i1, i2 in combinations(indices, 2):
                                matches.append((i1, i2, 'mode3', region, season))
                                match_count += 1

                pbar.set_postfix({"æ ¼ç½‘": f"{grid_id}", "å­£èŠ‚": season, "æ‰¾åˆ°": match_count})
                pbar.update(1)

        return matches

    def vectorized_matching_mode4(self, region_data: pd.DataFrame, region: str, target_seasons: Optional[List[str]] = None) -> List[Tuple]:
        """
        å‘é‡åŒ–æ¨¡å¼4åŒ¹é…ï¼ˆä¸åŒå­£èŠ‚ç›¸åå˜åŒ–ï¼‰- é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        matches = []

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œè¿‡æ»¤æ•°æ®
        if target_seasons and len(target_seasons) >= 2:
            region_data = region_data[region_data['Season'].isin(target_seasons)].copy()
            if len(region_data) == 0:
                self.logger.warning(f"âš ï¸ åœ¨{region}åŒºåŸŸä¸­æœªæ‰¾åˆ°æŒ‡å®šå­£èŠ‚çš„æ•°æ®")
                return matches

        # é¢„è®¡ç®—åŒºé—´
        rates = region_data['LAI_Change_Rate_mean'].values
        intervals = self.get_rate_interval_vectorized(rates, use_abs=True)
        region_data = region_data.copy()
        region_data['interval'] = intervals

        # æŒ‰grid_idåˆ†ç»„
        grouped = region_data.groupby('grid_id', observed=True)

        with tqdm(total=len(grouped),
                  desc=f"ğŸ”„ æ¨¡å¼4-ä¸åŒå­£èŠ‚ç›¸åå˜åŒ–({region})",
                  unit="æ ¼ç½‘") as pbar:

            for grid_id, group in grouped:
                if len(group) < 2:
                    pbar.update(1)
                    continue

                # åˆ†ç¦»ä¸åŒgrid_typeçš„è®°å½•
                type_1_records = group[group['grid_type'] == 1]
                type_neg1_records = group[group['grid_type'] == -1]

                if len(type_1_records) == 0 or len(type_neg1_records) == 0:
                    pbar.update(1)
                    continue

                match_count = 0

                # æŒ‰å­£èŠ‚åˆ†ç»„
                type_1_by_season = type_1_records.groupby('Season', observed=True)
                type_neg1_by_season = type_neg1_records.groupby('Season', observed=True)

                # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å­£èŠ‚ï¼Œåªå¤„ç†è¿™äº›å­£èŠ‚
                if target_seasons:
                    type_1_by_season = {s: g for s, g in type_1_by_season if s in target_seasons}
                    type_neg1_by_season = {s: g for s, g in type_neg1_by_season if s in target_seasons}

                # ä¸åŒå­£èŠ‚é—´çš„å‘é‡åŒ–åŒ¹é…
                for season1, group1 in type_1_by_season:
                    for season2, group2 in type_neg1_by_season:
                        if season1 == season2:
                            continue

                        # å‘é‡åŒ–åŒºé—´åŒ¹é…
                        for interval in range(len(self.rate_intervals) - 1):
                            g1_interval = group1[group1['interval'] == interval]
                            g2_interval = group2[group2['interval'] == interval]

                            if len(g1_interval) > 0 and len(g2_interval) > 0:
                                idx1_array = g1_interval.index.values
                                idx2_array = g2_interval.index.values

                                idx1_mesh, idx2_mesh = np.meshgrid(idx1_array, idx2_array)
                                for i1, i2 in zip(idx1_mesh.ravel(), idx2_mesh.ravel()):
                                    matches.append((i1, i2, 'mode4', region, season1, season2))
                                    match_count += 1

                pbar.set_postfix({"æ ¼ç½‘": f"{grid_id}", "æ‰¾åˆ°": match_count})
                pbar.update(1)

        return matches

    def _build_match_records_optimized(self, matches: List[Tuple], global_data: pd.DataFrame, mode: str) -> List[Dict]:
        """
        ä¼˜åŒ–ç‰ˆæ„å»ºåŒ¹é…è®°å½•ï¼Œå‡å°‘ç±»å‹è½¬æ¢å¼€é”€
        """
        match_records = []

        for match_tuple in matches:
            try:
                if len(match_tuple) == 5:  # ç›¸åŒå­£èŠ‚
                    idx1, idx2, mode_name, region, season = match_tuple
                    season1 = season2 = season
                    season_type = "same_season"
                    matched_season = season
                else:  # ä¸åŒå­£èŠ‚
                    idx1, idx2, mode_name, region, season1, season2 = match_tuple
                    season_type = "diff_season"
                    matched_season = None

                # è·å–å®Œæ•´è®°å½•
                record1 = global_data.iloc[idx1]
                record2 = global_data.iloc[idx2]

                # æ„å»ºå®Œæ•´çš„åŒ¹é…è®°å½•
                match_record = {}

                # è·å–æ‰€æœ‰åŸå§‹æ•°æ®å­—æ®µï¼ˆæ’é™¤æˆ‘ä»¬æ·»åŠ çš„è¾…åŠ©å­—æ®µï¼‰
                exclude_fields = {'interval', 'source_file', 'season_from_path', 'Season'}
                original_fields = [col for col in record1.index if col not in exclude_fields]

                # å¿«é€Ÿå¤åˆ¶å­—æ®µï¼Œå‡å°‘ç±»å‹è½¬æ¢å¼€é”€
                for field in original_fields:
                    # ç›´æ¥å¤åˆ¶ï¼Œå‡å°‘ç±»å‹è½¬æ¢å¼€é”€
                    value1 = record1[field]
                    value2 = record2[field]

                    # å¤„ç†categoryç±»å‹
                    if hasattr(value1, 'item'):
                        value1 = value1.item()
                    if hasattr(value2, 'item'):
                        value2 = value2.item()

                    match_record[f"{field}_1"] = value1
                    match_record[f"{field}_2"] = value2

                # è®¡ç®—LAIå˜åŒ–ç‡åŒºé—´ä¿¡æ¯
                rate1 = float(record1['LAI_Change_Rate_mean'])
                rate2 = float(record2['LAI_Change_Rate_mean'])
                use_abs = mode in ['mode1', 'mode4']

                # è®¡ç®—åŒºé—´
                interval1 = self.get_rate_interval_vectorized(np.array([rate1]), use_abs)[0]
                interval2 = self.get_rate_interval_vectorized(np.array([rate2]), use_abs)[0]

                # ç”ŸæˆåŒºé—´èŒƒå›´æè¿°
                def get_range_description(interval_idx, use_abs):
                    if interval_idx >= len(self.rate_intervals) - 1:
                        interval_idx = len(self.rate_intervals) - 2

                    min_val = self.rate_intervals[interval_idx]
                    max_val = self.rate_intervals[interval_idx + 1]

                    if max_val == np.inf:
                        return f"[{min_val}, +âˆ)"
                    else:
                        return f"[{min_val}, {max_val})"

                # æ·»åŠ åŒ¹é…åˆ†æä¿¡æ¯
                match_record.update({
                    # åŸºæœ¬åŒ¹é…ä¿¡æ¯
                    'matching_mode': mode_name,
                    'matching_region': region,
                    'season_type': season_type,
                    'matched_season': matched_season,
                    'season1': season1,
                    'season2': season2,

                    # å¹´ä»½å¯¹ä¿¡æ¯
                    'year_pair_1': str(record1.get('year_pair', '')),
                    'year_pair_2': str(record2.get('year_pair', '')),

                    # LAIå˜åŒ–ç‡åŒºé—´ä¿¡æ¯
                    'rate_interval_type': 'absolute' if use_abs else 'signed',
                    'rate_interval_1': int(interval1),
                    'rate_interval_2': int(interval2),
                    'rate_range_1': get_range_description(interval1, use_abs),
                    'rate_range_2': get_range_description(interval2, use_abs),

                    # æ•°å€¼åˆ†æä¿¡æ¯
                    'lai_change_rate_1': rate1,
                    'lai_change_rate_2': rate2,
                    'rate_diff': abs(rate1 - rate2),
                    'grid_type_1': int(record1['grid_type']),
                    'grid_type_2': int(record2['grid_type'])
                })

                match_records.append(match_record)

            except Exception as e:
                self.logger.warning(f"âš ï¸ æ„å»ºåŒ¹é…è®°å½•å¤±è´¥: {e}")
                continue

        return match_records

    def group_matches_by_season(self, matches: List[Tuple], mode: str) -> Dict[str, List[Tuple]]:
        """
        æŒ‰å­£èŠ‚åˆ†ç»„åŒ¹é…ç»“æœ
        """
        season_groups = defaultdict(list)

        for match_tuple in matches:
            if len(match_tuple) == 5:  # ç›¸åŒå­£èŠ‚åŒ¹é…ï¼š(idx1, idx2, mode_name, region, season)
                season_key = match_tuple[4]  # season
            else:  # ä¸åŒå­£èŠ‚åŒ¹é…ï¼š(idx1, idx2, mode_name, region, season1, season2)
                season1, season2 = match_tuple[4], match_tuple[5]
                # åˆ›å»ºå­£èŠ‚å¯¹çš„é”®ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åºç¡®ä¿ä¸€è‡´æ€§
                season_key = "_".join(sorted([season1, season2]))

            season_groups[season_key].append(match_tuple)

        return dict(season_groups)

    def export_matches_by_season(self, matches: List[Tuple], global_data: pd.DataFrame,
                                mode: str, region: str) -> int:
        """
        æŒ‰å­£èŠ‚åˆ†ç»„å¯¼å‡ºåŒ¹é…ç»“æœ
        """
        if not matches:
            self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ")
            return 0

        self.logger.info(f"ğŸ’¾ å¼€å§‹æŒ‰å­£èŠ‚åˆ†ç»„å¯¼å‡º {mode} åŒ¹é…ç»“æœ...")

        # åˆ›å»ºåŒºåŸŸç›®å½•
        region_dir = self.output_dir / region
        region_dir.mkdir(exist_ok=True)

        # æŒ‰å­£èŠ‚åˆ†ç»„
        season_groups = self.group_matches_by_season(matches, mode)

        total_exported = 0
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        for season_key, season_matches in season_groups.items():
            if not season_matches:
                continue

            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{season_key}_{mode}_{timestamp}.csv"
            output_file = region_dir / filename

            # æ„å»ºè¯¥å­£èŠ‚ç»„çš„åŒ¹é…è®°å½•
            match_records = self._build_match_records_optimized(season_matches, global_data, mode)

            if match_records:
                # å†™å…¥æ–‡ä»¶
                df = pd.DataFrame(match_records)
                df.to_csv(output_file, index=False, float_format='%.6f')

                count = len(match_records)
                total_exported += count
                self.logger.info(f"âœ… å¯¼å‡º {season_key} å­£èŠ‚ {count} å¯¹åŒ¹é…åˆ°: {output_file}")
            else:
                self.logger.warning(f"âš ï¸ {season_key} å­£èŠ‚æ²¡æœ‰æœ‰æ•ˆçš„åŒ¹é…è®°å½•")

        self.logger.info(f"ğŸ‰ æ€»å…±å¯¼å‡º {total_exported} å¯¹åŒ¹é…ç»“æœï¼Œåˆ†å¸ƒåœ¨ {len(season_groups)} ä¸ªå­£èŠ‚æ–‡ä»¶ä¸­")
        return total_exported

    def run_specific_matching(self, global_data: pd.DataFrame, target_region: str,
                            target_mode: str, season_param: Optional[str] = None) -> int:
        """
        æ‰§è¡Œç‰¹å®šçš„åŒ¹é…åˆ†æ
        """
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œç‰¹å®šåŒ¹é…åˆ†æ...")
        self.logger.info(f"   ç›®æ ‡åŒºåŸŸ: {target_region}")
        self.logger.info(f"   åŒ¹é…æ¨¡å¼: {target_mode}")
        self.logger.info(f"   å­£èŠ‚å‚æ•°: {season_param}")

        # è¿‡æ»¤ç›®æ ‡åŒºåŸŸæ•°æ®
        region_mask = global_data['region'] == target_region
        region_data = global_data[region_mask].copy()

        if len(region_data) == 0:
            self.logger.error(f"âŒ åœ¨åŒºåŸŸ {target_region} ä¸­æœªæ‰¾åˆ°æ•°æ®")
            return 0

        self.logger.info(f"ğŸŒ² ç›®æ ‡åŒºåŸŸæ•°æ®é‡: {len(region_data):,} æ¡è®°å½•")

        # åŒ¹é…å‡½æ•°æ˜ å°„
        matching_functions = {
            'mode1': self.vectorized_matching_mode1,
            'mode2': self.vectorized_matching_mode2,
            'mode3': self.vectorized_matching_mode3,
            'mode4': self.vectorized_matching_mode4
        }

        if target_mode not in matching_functions:
            self.logger.error(f"âŒ ä¸æ”¯æŒçš„åŒ¹é…æ¨¡å¼: {target_mode}")
            return 0

        match_func = matching_functions[target_mode]

        # æ ¹æ®ä¸åŒæ¨¡å¼å¤„ç†å­£èŠ‚å‚æ•°
        try:
            if target_mode in ['mode1', 'mode3']:  # ç›¸åŒå­£èŠ‚åŒ¹é…
                matches = match_func(region_data, target_region, season_param)
            elif target_mode in ['mode2', 'mode4']:  # ä¸åŒå­£èŠ‚åŒ¹é…
                if season_param and ',' in season_param:
                    target_seasons = [s.strip() for s in season_param.split(',')]
                    matches = match_func(region_data, target_region, target_seasons)
                else:
                    matches = match_func(region_data, target_region)
            else:
                matches = match_func(region_data, target_region)

            # æŒ‰å­£èŠ‚åˆ†ç»„å¯¼å‡ºç»“æœ
            match_count = self.export_matches_by_season(matches, global_data, target_mode, target_region)

            self.logger.info(f"âœ… åŒ¹é…åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {match_count} å¯¹åŒ¹é…")
            return match_count

        except Exception as e:
            self.logger.error(f"âŒ åŒ¹é…è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0


def get_user_input(available_regions: List[str], available_seasons: List[str]) -> Tuple[str, str, Optional[str]]:
    """
    è·å–ç”¨æˆ·è¾“å…¥å‚æ•°
    """
    print("\n" + "="*60)
    print("ğŸŒ LAIæ ¼ç½‘åŒ¹é…åˆ†æå™¨ - ç”¨æˆ·å®šåˆ¶ç‰ˆ (æŒ‰å­£èŠ‚åˆ†åˆ«å¯¼å‡º)")
    print("="*60)

    # æ˜¾ç¤ºå¯ç”¨åŒºåŸŸ
    print(f"\nğŸ“ å¯ç”¨çš„ç”Ÿæ€åŒºåŸŸ ({len(available_regions)} ä¸ª):")
    for i, region in enumerate(available_regions, 1):
        print(f"   {i}. {region}")

    # é€‰æ‹©åŒºåŸŸ
    while True:
        try:
            region_choice = input(f"\nè¯·é€‰æ‹©åŒºåŸŸ (1-{len(available_regions)}): ").strip()
            region_idx = int(region_choice) - 1
            if 0 <= region_idx < len(available_regions):
                target_region = available_regions[region_idx]
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

    # æ˜¾ç¤ºåŒ¹é…æ¨¡å¼
    modes = {
        1: ("mode1", "ç›¸åŒå­£èŠ‚å†…çš„ç›¸åå˜åŒ–åŒ¹é…"),
        2: ("mode2", "ä¸åŒå­£èŠ‚å†…çš„ç›¸åŒå˜åŒ–åŒ¹é…"),
        3: ("mode3", "ç›¸åŒå­£èŠ‚å†…çš„ç›¸åŒå˜åŒ–åŒ¹é…"),
        4: ("mode4", "ä¸åŒå­£èŠ‚å†…çš„ç›¸åå˜åŒ–åŒ¹é…")
    }

    print(f"\nğŸ” å¯ç”¨çš„åŒ¹é…æ¨¡å¼:")
    for num, (mode_code, description) in modes.items():
        print(f"   {num}. {mode_code}: {description}")

    # é€‰æ‹©æ¨¡å¼
    while True:
        try:
            mode_choice = input(f"\nè¯·é€‰æ‹©åŒ¹é…æ¨¡å¼ (1-4): ").strip()
            mode_idx = int(mode_choice)
            if mode_idx in modes:
                target_mode, mode_desc = modes[mode_idx]
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

    # å­£èŠ‚å‚æ•°è¾“å…¥
    print(f"\nğŸŒ¿ å¯ç”¨çš„å­£èŠ‚: {', '.join(available_seasons)}")

    season_param = None
    if target_mode in ['mode1', 'mode3']:  # ç›¸åŒå­£èŠ‚åŒ¹é…
        print(f"\nâš ï¸  {target_mode} æ˜¯ç›¸åŒå­£èŠ‚åŒ¹é…æ¨¡å¼")
        print("ğŸ“‹ æ³¨æ„ï¼šç»“æœå°†æŒ‰æ¯ä¸ªå­£èŠ‚åˆ†åˆ«å¯¼å‡ºåˆ°ä¸åŒçš„CSVæ–‡ä»¶")
        season_input = input("è¯·è¾“å…¥ç›®æ ‡å­£èŠ‚ (ç•™ç©ºè¡¨ç¤ºæ‰€æœ‰å­£èŠ‚): ").strip()
        if season_input and season_input in available_seasons:
            season_param = season_input
        elif season_input:
            print(f"âš ï¸ å­£èŠ‚ '{season_input}' ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨æ‰€æœ‰å­£èŠ‚")

    elif target_mode in ['mode2', 'mode4']:  # ä¸åŒå­£èŠ‚åŒ¹é…
        print(f"\nâš ï¸  {target_mode} æ˜¯ä¸åŒå­£èŠ‚åŒ¹é…æ¨¡å¼")
        print("ğŸ“‹ æ³¨æ„ï¼šç»“æœå°†æŒ‰å­£èŠ‚å¯¹åˆ†åˆ«å¯¼å‡ºåˆ°ä¸åŒçš„CSVæ–‡ä»¶ (å¦‚: Spring_Summer_mode2.csv)")
        season_input = input("è¯·è¾“å…¥ç›®æ ‡å­£èŠ‚ç»„åˆ (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚ Spring,Summerï¼Œç•™ç©ºè¡¨ç¤ºæ‰€æœ‰å­£èŠ‚): ").strip()
        if season_input:
            seasons = [s.strip() for s in season_input.split(',')]
            valid_seasons = [s for s in seasons if s in available_seasons]
            if len(valid_seasons) >= 2:
                season_param = ','.join(valid_seasons)
            else:
                print(f"âš ï¸ éœ€è¦è‡³å°‘2ä¸ªæœ‰æ•ˆå­£èŠ‚ï¼Œå°†ä½¿ç”¨æ‰€æœ‰å­£èŠ‚")

    # ç¡®è®¤å‚æ•°
    print(f"\nâœ… å‚æ•°ç¡®è®¤:")
    print(f"   ç›®æ ‡åŒºåŸŸ: {target_region}")
    print(f"   åŒ¹é…æ¨¡å¼: {target_mode} ({mode_desc})")
    print(f"   å­£èŠ‚å‚æ•°: {season_param if season_param else 'æ‰€æœ‰å­£èŠ‚'}")
    print(f"   å¯¼å‡ºæ–¹å¼: æŒ‰å­£èŠ‚åˆ†åˆ«å¯¼å‡ºåˆ°ä¸åŒçš„CSVæ–‡ä»¶")

    confirm = input(f"\nç¡®è®¤æ‰§è¡ŒåŒ¹é…? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        exit(0)

    return target_region, target_mode, season_param


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    input_directory = r"D:\article\SynologyDrive\LAI-LST-Asymmetric\data\outCSV\00_ori"  # æ•°æ®æ ¹ç›®å½•è·¯å¾„
    output_directory = r"D:\article\SynologyDrive\LAI-LST-Asymmetric\data\outCSV\01_matching_results"  # è¾“å‡ºç›®å½•

    try:
        # åˆ›å»ºä¼˜åŒ–ç‰ˆåˆ†æå™¨å®ä¾‹
        analyzer = OptimizedGridMatchingAnalyzer(
            input_dir=input_directory,
            output_dir=output_directory,
            chunk_size=50000  # å¯æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´ï¼š30000(8GB), 50000(16GB), 100000(32GB+)
        )

        # åŠ è½½æ•°æ®
        global_data = analyzer.load_all_data_parallel()

        # è·å–å¯ç”¨çš„åŒºåŸŸå’Œå­£èŠ‚
        available_regions = sorted(analyzer.stats['regions'])
        available_seasons = sorted(analyzer.stats['seasons'])

        # è·å–ç”¨æˆ·è¾“å…¥
        target_region, target_mode, season_param = get_user_input(available_regions, available_seasons)

        # æ‰§è¡Œç‰¹å®šåŒ¹é…åˆ†æ
        match_count = analyzer.run_specific_matching(global_data, target_region, target_mode, season_param)

        print(f"\nğŸ‰ åŒ¹é…åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {match_count} å¯¹åŒ¹é…")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶æŒ‰å­£èŠ‚åˆ†åˆ«ä¿å­˜åœ¨: {analyzer.output_dir}/{target_region}/")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()